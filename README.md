# Student Proud Project, Milestone 3

This project implements and evaluates three major Deep Learning (DL) model architectures—**VGG-16, DenseNet201, and Xception**—in both standalone and hybrid configurations to classify CT kidney scan images into four categories: Normal, Cyst, Stone, and Tumor. The goal is to replicate and exceed the performance benchmarks from the reference paper, **[Enhanced Automatic Identification of Kidney Cyst, Stone and Tumor using Deep Learning](https://ieeexplore.ieee.org/document/10617000).**

## 1. Project Overview

The project expanded in Milestone 3 to include a comparative study of deep architectures (DenseNet201, Xception) and a rigorous optimizer ablation (Adam, RMSprop, SGD) to push classification accuracy towards the target of 99.76%. The highest performing model was the **Hybrid DenseNet201 + XGBoost**.

| Model | Optimizer/Classifier | Achieved Accuracy | Paper's Target Accuracy | Result |
| :--- | :--- | :--- | :--- | :--- |
| Hybrid (Best Model) | **DenseNet201 + XGBoost** | **97.15%** | 99.76% | **Highest Achieved**
| Standalone (Best CNN) | VGG-16 + Adam | 96.67% | 90.99% | **Exceeded Benchmark**
| Standalone (Deep CNN) | Xception + Adam | 95.90% | 90.99% | Exceeded Benchmark

## 2. Setup and Installation

### Prerequisites

To run the provided Google Colab Notebook (`milestone3.ipynb`), you need the following:

1.  **Google Account and Google Colab:** The notebook is designed to run in a Google Colab environment, utilizing its GPU capabilities.
2.  **Dataset:** The raw dataset must be downloaded and placed into your Google Drive.

### 2.1 Dataset Download

The project relies on the following Kaggle dataset:

> **[CT Kidney Dataset: Normal, Cyst, Tumor, and Stone](https://www.kaggle.com/datasets/nazmul0087/ct-kidney-dataset-normal-cyst-tumor-and-stone/data)**

**Before running any code, download this dataset, and place the unzipped contents into your Google Drive.**

### 2.2 Running the Notebook

1.  Open the `milestone2.ipynb` file in Google Colab.
2.  Ensure your Colab runtime is set to **GPU** (Runtime -> Change runtime type).
3.  Execute the first code cell to mount your Google Drive. This is **essential** for accessing the dataset:

    ```python
    !cp -r /content/drive/MyDrive/CTKidneyDataset /content/
    ```

    **(Note: If your dataset path is different, adjust this copy command accordingly.)**
4.  Run all subsequent code cells sequentially.

## 3. Key Model Implementation Details

### Standalone CNN Models (VGG-16, DenseNet201, Xception)

These models were used to establish baseline performance and test the effectiveness of adaptive optimizers.

- **Architecture & Input:**
  - **VGG-16**: Standard $224 \times 224$ input.
  - **DenseNet201**: Standard $224 \times 224$ input.
  - **Xception**: Required $299 \times 299$ input size.

**Ablation Study**: Each architecture was tested with Adam, RMSprop, and SGD optimizers to determine the most effective convergence strategy. Adam consistently performed best across all models.

**Tuning Strategy**: Fine-tuning was achieved using extremely low learning rates (typically $10^{-6}$ or lower) after training the new classification head.

### Hybrid Feature Extraction Models (Best Performer: DenseNet201 + XGBoost)

The hybrid models leverage the CNN base as a feature extractor, replacing the final dense classification layer with the powerful XGBoost ensemble classifier.

- **Feature Extraction Process**: The frozen base of the CNN is used to generate feature vectors, which are then used as input to XGBoost.

- **Memory Optimization**: The final convolutional output for all hybrid models was passed through a Global Average Pooling 2D (GAP) layer to drastically reduce the feature vector size and prevent out-of-memory errors during the loading and training of the XGBoost classifier.

- **Success of DenseNet + XGBoost**: The highest accuracy was achieved by this hybrid, demonstrating that XGBoost was best able to leverage the rich, multi-scale features generated by the DenseNet architecture's dense connectivity pattern.

## 4. Final Performance Visualization

The project generated detailed metrics and visualizations, which can be found in the notebook's output cells:

* **Confusion Matrices:** Show the true number of misclassifications for each model.
* **Classification Reports:** Provide Precision, Recall, and F1-scores for each of the four classes.
* **Precision-Recall Curves:** Offer a robust visualization of the model's performance on minority classes.
